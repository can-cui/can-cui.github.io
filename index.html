<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Can CUI - Resume</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            display: flex;
        }

        #left-column {
            flex: 2;
            margin-right: 20px;
            text-align: center;
            background-color: rgb(200, 237, 250);
            padding: 20px;
            position: sticky;
            top: 0;
        }

        #right-column {
            flex: 8;
            background-color: rgb(250, 242, 200); /* Light yellow background color for the right column */
            padding: 20px;
            overflow-y: auto; /* Make the right column scrollable */
            max-height: 100vh; /* Set maximum height to full viewport height */
        }

        img {
            border-radius: 50%;
        max-width: 300px; /* Adjust the max-width value as needed */
        width: 100%; /* Ensures the image takes up the available width */
        }

        section {
            margin-top: 20px;
            margin-bottom: 50px; 
        }

        h2 {
            color: #333;
        }

        ul {
            list-style-type: none;
            margin: 0;
            padding: 0;
        }

        li {
            margin-bottom: 10px;
        }
    </style>
</head>

<body>
    <div id="left-column">
    <header>
        <h1>Can CUI</h1>
        <h2>PhD in Computer Science</h2>
            <!--, from <a href="https://team.inria.fr/multispeech/" target="_blank">MULTISPEECH</a> team of <a href="https://www.inria.fr/fr" target="_blank">Inria</a> in France</h2>-->
        <img src="profile.JPG" alt="Your Photo">
    </header>

    <section id="contact">
        <p>can.cui@inria.fr</p>
        <p>+33 (0) 659337638</p>
        <p><i>Note: You can speak to me in Chinese (native), French (C2), English (B2), and eventually German (A2) just for fun ;-)</i></p>
        <p><a href="https://www.linkedin.com/in/can-cui-">LinkedIn</a></p>
        <p><a href="https://github.com/can-cui">GitHub</a></p>
        <p><a href="https://scholar.google.com/citations?user=b0TbxnwAAAAJ&hl=en">Google Scholar</a></p>
        <p><a href="CUI_en_cv.pdf" target="_blank">Curriculum Vitae</a></p>
    </section>
    </div>

    
    <div id="right-column">
        <section id="aboutme">
        <h2>About Me</h2>
        <ul>
            <li>
                <p>Hello! I'm Can CUI, originally from China and currently a researcher at iFLYTEK, where I focus on multilingual text-to-speech synthesis and multilingual voice cloning. I hold a Ph.D. from the Multispeech team at Inria Nancy in France, where I was supervised by Emmanuel Vincent and Mostafa Sadeghi. My doctoral research explored the fascinating realms of multichannel multi-speaker speech recognition, including automatic speech recognition (ASR), speech separation, speaker diarization, and voice activity detection. Feel free to explore more about my journey and research endeavors on this page!</p>
            </li>
        </ul>
    </section>

    <section id="education">
        <h2>Education</h2>
        <ul>
            
            <li>
                <strong>Ph.D. in Computer Science</strong> - Université de Lorraine, Nancy, France (10 2021 - 10 2024)
                <p>I am engaged in the research and development at iFLYTEK of multilingual voice cloning, focusing primarily on one-shot voice cloning. Using just 3 seconds of speech from a speaker the model has never encountered, it is possible to replicate the speaker's speech content across different languages.</p>
            </li>
            <li>
                <strong>Master in Language and Computer Science</strong> - Sorbonne Université, Paris, France (9 2019 - 9 2021)
                <p>Courses: Speech recognition, emotion recognition, creation of generative language model, etc.</p>
            </li>
            <li>
                <strong>Master in General French Linguistics</strong> - Sorbonne Université, Paris, France (9 2018 - 9 2020)
            </li>
            <li>
                <strong>Bachelor in French language and literature</strong> - University of Yunnan, Kunming, China (9 2013 - 9 2017)
            </li>
        </ul>
    </section>

    <section id="experience">
        <h2>Work Experience</h2>
        <ul>
            <li>
                <strong>Research Scientist in Text-to-Speech</strong> - iFLYTEK Co. Ltd., Shanghai, China (11 2024 - Present)
                <p>Thesis Title: Joint embedded speech separation, diarization and recognition for the automatic generation of meeting minutes.</p>
                <p>Researche domains: Multichannel speech separation, speaker diarization, automatic speech recognition (ASR), multichannel speaker-attributed ASR, voice activity detection (VAD).</p>
            </li>
            <li>
                <strong>PhD candidate</strong> - Inria, Nancy, France (10 2021 - Present)
                <p>The automatic generation of meeting minutes thus involves solving a set of tasks: i) segmenting the signal according to the number of speakers and who is speaking at each time (diarisation), ii) separating overlapping speech signals [3] and enhancing them with respect to ambient noise and reverberation, iii) ensuring the robustness of ASR with respect to diarization errors and signal distortions introduced by separation and enhancement, and iv) removing disfluencies from the word-for-word transcription in order to obtain readable minutes. The objective of this PhD is to design a system which can jointly address the first three tasks given a single-channel or a multichannel signal and which can be embedded in a device with limited computing power (for example a mobile phone), while being able to compete with current Cloud-based technologies.</p>
            </li>
            <li>
                <strong>R&D researcher</strong> - Vivoka, Metz, France (8 2022 - 2 2024)
                <p>Research into the Dictation solution: automatic speech recognition with punctuation; creation of a proof of concept for Dictation; creation of a proof of concept for automatic transcription of multi-channel and multi-speaker meetings.</p>
            </li>
            <li>
                <strong>AI Research Intern</strong> - Orange, Paris, France (3 2021 - 8 2021)
                <p>Subject: Detection of weak signals within customer conversations in a call center. Activities: Transformation and construction of abstract and extractive summaries; Classifications of conversations with regard to the emotions within the conversations; Extraction of themes from conversations</p>
            </li>
        </ul>
    </section>
<!--
    <section id="preprints">
        <h2>Preprints</h2>

        <ul>
            <li>
                <em>Can Cui, Imran Sheikh, Mostafa Sadeghi, Emmanuel Vincent, </em>
                <strong>End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data, </strong>
                <em>November 2023.</em>
            <a href="https://arxiv.org/abs/2311.17741">[Paper]</a></a>
            </li>
            <!-- Add more publications as needed 
        </ul>
    </section>
-->
     <section id="conference">
        <h2>Conference Papers</h2>
         <ul>
            <li>
                <em>Can Cui, Imran Sheikh, Mostafa Sadeghi, Emmanuel Vincent, </em>
                <strong>Joint Beamforming and Speaker-Attributed ASR for Real Distant-Microphone Meeting Transcription, </strong>
                <em>Proc. of the IEEE European Signal Processing Conference (EUSIPCO 2025), Sep 2025, Palermo, Italy. </em>
            <a href="https://arxiv.org/abs/2410.21849">[Paper]</a></a>
             <a href="https://github.com/can-cui/joint-beamforming-sa-asr">[Code]</a></a>
            </li>
            <!-- Add more publications as needed -->
        </ul>

        <ul>
            <li>
                <em>Can Cui, Imran Sheikh, Mostafa Sadeghi, Emmanuel Vincent, </em>
                <strong>End-to-end Joint Punctuated and Normalized  ASR with a Limited Amount of Punctuated Training Data, </strong>
                <em>Proc. of the IEEE European Signal Processing Conference (EUSIPCO 2025), Sep 2025, Palermo, Italy. </em>
            <a href="https://arxiv.org/abs/2311.17741">[Paper]</a></a>
            <a href="https://github.com/can-cui/punctuated-normalized-asr">[Code]</a></a>
            </li>
            <!-- Add more publications as needed -->
        </ul>

        <ul>
            <li>
                <em>Taous Iatariene, Can Cui, Alexandre Guérin, Romain Serizel, </em>
                <strong>Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers, </strong>
                <em>Proc. of the IEEE European Signal Processing Conference (EUSIPCO 2025), Sep 2025, Palermo, Italy. </em>
            <a href="https://arxiv.org/abs/2506.19875">[Paper]</a></a>
            </li>
            <!-- Add more publications as needed -->
        </ul>
         
        <ul>
            <li>
                <em>Can Cui, Imran Sheikh, Mostafa Sadeghi, Emmanuel Vincent, </em>
                <strong>Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting Applications, </strong>
                <em>Proc. of the IEEE Odyssey-The Speaker and Language Recognition Workshop, June 2024, Quebec, Canada. </em>
            <a href="https://arxiv.org/abs/2403.06570">[Paper]</a></a>
            <a href="https://arxiv.org/abs/2410.21849">[Code]</a></a>
            </li>
            <!-- Add more publications as needed -->
        </ul>
   
        <ul>
            <li>
                <em>Can Cui, Imran Sheikh, Mostafa Sadeghi, Emmanuel Vincent, </em>
                <strong>End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder and Input Feature Analysis, </strong>
                <em>Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), December 2023, Taibei, China. </em>
            <a href="https://arxiv.org/abs/2310.10106">[Paper]</a></a>
            <a href="https://arxiv.org/abs/2410.21849">[Code]</a></a>
            </li>
            <!-- Add more publications as needed -->
        </ul>
    </section>
</div>

</body>

</html>
