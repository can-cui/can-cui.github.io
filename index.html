<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Can CUI - Resume</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            display: flex;
        }

        #left-column {
            flex: 2;
            margin-right: 20px;
            text-align: center;
            background-color: rgb(200, 237, 250);
            padding: 20px;
            position: sticky;
            top: 0;
        }

        #right-column {
            flex: 8;
            background-color: rgb(250, 242, 200); /* Light yellow background color for the right column */
            padding: 20px;
            overflow-y: auto; /* Make the right column scrollable */
            max-height: 100vh; /* Set maximum height to full viewport height */
        }

        img {
            border-radius: 50%;
        max-width: 300px; /* Adjust the max-width value as needed */
        width: 100%; /* Ensures the image takes up the available width */
        }

        section {
            margin-top: 20px;
            margin-bottom: 50px; 
        }

        h2 {
            color: #333;
        }

        ul {
            list-style-type: none;
            margin: 0;
            padding: 0;
        }

        li {
            margin-bottom: 10px;
        }
    </style>
</head>

<body>
    <div id="left-column">
    <header>
        <h1>Can CUI</h1>
        <h2>PhD Candidate in Computer Science</h2>
            <!--, at <a href="https://team.inria.fr/multispeech/" target="_blank">MULTISPEECH</a> team of <a href="https://www.inria.fr/fr" target="_blank">Inria</a> in France</h2>-->
        <img src="profile.JPG" alt="Your Photo">
    </header>

    <section id="contact">
        <p>Nationality: Chinese</p>
        <p>Email: cuicanenfrance@gmail.com</p>
        <p>Phone: +33 (0) 659337638</p>
        <p><a href="https://www.linkedin.com/in/can-cui-">LinkedIn</a></p>
        <p><a href="https://github.com/can-cui">GitHub</a></p>
        <p><a href="https://scholar.google.com/citations?user=b0TbxnwAAAAJ&hl=en">Google Scholar</a></p>
        <p><a href="CUI_en_cv.pdf" target="_blank">Curriculum Vitae</a></p>
    </section>
    </div>

    
    <div id="right-column">
    <section id="education">
        <h2>Education</h2>
        <ul>
            <li>
                <strong>Ph.D. in Computer Science</strong> - Université de Lorraine, Nancy, France (10 2021 - Present)
                <p>Thesis Title: Joint embedded speech separation, diarization and recognition for the automatic generation of meeting minutes.</p>
                <p>Researche domains: Multichannel speech separation, speaker diarization, automatic speech recognition (ASR), multichannel speaker-attributed ASR, voice activity detection (VAD).</p>
            </li>
            <li>
                <strong>Master in Language and Computer Science</strong> - Sorbonne Université, Paris, France (9 2019 - 9 2021)
                <p>Courses: Speech recognition, emotion recognition, creation of generative language model, etc.</p>
            </li>
            <li>
                <strong>Master in General French Linguistics</strong> - Sorbonne Université, Paris, France (9 2018 - 9 2020)
            </li>
            <li>
                <strong>Bachelor in French language and literature</strong> - University of Yunnan, Kunming, China (9 2013 - 9 2017)
            </li>
        </ul>
    </section>

    <section id="experience">
        <h2>Work Experience</h2>
        <ul>
            <li>
                <strong>PhD candidate</strong> - Inria, Nancy, France (10 2021 - Present)
                <p>The automatic generation of meeting minutes thus involves solving a set of tasks: i) segmenting the signal according to the number of speakers and who is speaking at each time (diarisation), ii) separating overlapping speech signals [3] and enhancing them with respect to ambient noise and reverberation, iii) ensuring the robustness of ASR with respect to diarization errors and signal distortions introduced by separation and enhancement, and iv) removing disfluencies from the word-for-word transcription in order to obtain readable minutes. The objective of this PhD is to design a system which can jointly address the first three tasks given a single-channel or a multichannel signal and which can be embedded in a device with limited computing power (for example a mobile phone), while being able to compete with current Cloud-based technologies.</p>
            </li>
            <li>
                <strong>R&D researcher</strong> - Vivoka, Metz, France (8 2022 - 2 2024)
                <p>Research into the Dictation solution: automatic speech recognition with punctuation; creation of a proof of concept for Dictation; creation of a proof of concept for automatic transcription of multi-channel and multi-speaker meetings.</p>
            </li>
            <li>
                <strong>AI Research Intern</strong> - Orange, Paris, France (3 2021 - 8 2021)
                <p>Subject: Detection of weak signals within customer conversations in a call center. Activities: Transformation and construction of abstract and extractive summaries; Classifications of conversations with regard to the emotions within the conversations; Extraction of themes from conversations</p>
            </li>
        </ul>
    </section>

    <section id="preprints">
        <h2>Preprints</h2>
        <ul>
            <li>
                <em>Can Cui, Imran Sheikh, Mostafa Sadeghi, Emmanuel Vincent, </em>
                <strong>End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data, </strong>
                <em>November 2023.</em>
            <a href="https://arxiv.org/abs/2311.17741">[Paper]</a></a>
            </li>
            <!-- Add more publications as needed -->
        </ul>
    </section>

    <section id="conference">
        <h2>Conference Paper</h2>
        <ul>
            <li>
                <em>Can Cui, Imran Sheikh, Mostafa Sadeghi, Emmanuel Vincent, </em>
                <strong>End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder and Input Feature Analysis, </strong>
                <em>2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), </em>
                <em>Taipei, Taiwan, December 2023. </em>
            <a href="https://arxiv.org/abs/2310.10106">[Paper]</a></a>
            </li>
            <!-- Add more publications as needed -->
        </ul>
    </section>
</div>

</body>

</html>
